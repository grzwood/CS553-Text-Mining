Decision trees are special decision rules that are organized into a tree strictire. A decision tree divides the document space into non-overlapping regions at its leaves, and predictions are made at each leaf.Look at the figure of decision_tree_cst in the images folder. You can see that root with node id=1 contains the terms, and now for each term checks if the threshold is less than 0.0367 or greater than that value. If the value is less than 0.0365 or the value is missing it goes to left child. Where it further checks the value, and so on as it reaches the leaf node, a decision is made. One can easily convert a decision tree into a set of non-overlapping decision rules. 
In text topic node, the process discovers various topics  but assign each document to zero or more topics. This is different than text cluster node where it assigns each document to one of the different topics where in text topic, each document can be in zero or more topics.

In ROC chart- the greater the area under the curve, the better the model is. The brown line represents the baseline to which other models are compared. The blue line represents how well the decision tree-trb model did at predicting the target serious. This model used input from the text rule builder node. The green line represents how well the decision tree model did at predicting the target serious. The red line represents how well the decision tree-cst model did at predicting the target serious. Both the decision tree and the decision tree-cst models performed better thatn the trb model and both of them also performed similarly with respect to each other. The results of the model comparision node also have 3 other panels- output, fit stats, and score rankings.
The output displays the variable summary etc in training output. The fit statistics displays the model description and predecessor node of all the decision nodes and and the sum of frequencies, misclassification rate etc in a table. The 3rd panel displays a graph between cumulative lift and depth of the three decision tree nodes and as you can see, the trb has lower lift than cst and decision tree at respective depths.

The difference between results of decision tree node and text topic node are- text topic node displays the stats/facts regarding the terms and their topics, like topics panel that displays the category if it is simple or multiple, topics and the number of docs that topic id contains along with number of terms. Then there is number of terms by topics and number of documents by topics graph. In decision tree node, there is a decision tree, leaf stats, fit stats, score rankings overlay which displays the cumulative lift vs depth graph of each tree node and the last it contains an output panel.

There are various images like text rule builder where serious is set to "yes", text topic, text topic cst and text topic trb, tree graph when serious is set to "yes", roc chart, model comparision of all three decision trees, english rule when serious="yes", the whole diagram, change target values where target variable=serious and its value is set to "yes" with posterior probability for text value.